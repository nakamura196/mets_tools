{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# import collections\n",
    "import datetime\n",
    "# import fnmatch\n",
    "import math\n",
    "import os\n",
    "# import sys\n",
    "from lxml import etree, objectify\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def convert_size(size):\n",
    "    # convert size to human-readable form\n",
    "    size_name = (\"bytes\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\", \"EB\", \"ZB\", \"YB\")\n",
    "    i = int(math.floor(math.log(size,1024)))\n",
    "    p = math.pow(1024,i)\n",
    "    s = round(size/p)\n",
    "    s = str(s)\n",
    "    s = s.replace('.0', '')\n",
    "    return '{} {}'.format(s,size_name[i])\n",
    "\n",
    "\n",
    "class METSFile(object):\n",
    "    \"\"\"\n",
    "    Class for METS file parsing methods\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path):\n",
    "        # , dip_id, nickname\n",
    "        self.path = os.path.abspath(path)\n",
    "        # self.dip_id = dip_id\n",
    "        # self.nickname = nickname\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.path\n",
    "\n",
    "    def parse_dc(self, root):\n",
    "        \"\"\"\n",
    "        Parse SIP-level Dublin Core metadata into dc_model dictionary.\n",
    "        Based on parse_dc function from Archivematica parse_mets_to_db.py script:\n",
    "\n",
    "        https://github.com/artefactual/archivematica/blob/92d7abd238585e64e6064bc3f1ddfc663c4d3ace/\n",
    "        src/MCPClient/lib/clientScripts/parse_mets_to_db.py\n",
    "        \"\"\"\n",
    "        # Parse DC\n",
    "        dmds = root.xpath('dmdSec/mdWrap[@MDTYPE=\"DC\"]/parent::*')\n",
    "        dcmetadata = []\n",
    "        \n",
    "        # Find which DC to parse\n",
    "        if len(dmds) > 0:\n",
    "            # Want most recently updated\n",
    "            dmds = sorted(dmds, key=lambda e: e.get('CREATED', \"\"))\n",
    "            # Only want SIP DC, not file DC\n",
    "            div = root.find('structMap/div/div[@TYPE=\"Directory\"][@LABEL=\"objects\"]')\n",
    "            dmdids = div.get('DMDID')\n",
    "            # No SIP DC\n",
    "            if dmdids is None:\n",
    "                return\n",
    "            dmdids = dmdids.split()\n",
    "            for dmd in dmds[::-1]:  # Reversed\n",
    "                if dmd.get('ID', \"\") in dmdids:\n",
    "                    dc_xml = dmd.find('mdWrap/xmlData/dublincore')\n",
    "                    break\n",
    "            for elem in dc_xml:\n",
    "                dc_element = dict()\n",
    "                dc_element['element'] = elem.tag\n",
    "                dc_element['value'] = elem.text\n",
    "                if not dc_element['value'] is None:\n",
    "                    dcmetadata.append(dc_element)\n",
    "            return dcmetadata\n",
    "\n",
    "    def parse_mets(self):\n",
    "        \"\"\"\n",
    "        Parse METS file and save data to METS model\n",
    "        \"\"\"\n",
    "        # create list\n",
    "        original_files = []\n",
    "        original_file_count = 0\n",
    "\n",
    "        # get METS file name\n",
    "        mets_filename = os.path.basename(self.path)\n",
    "\n",
    "        # open xml file and strip namespaces\n",
    "        tree = etree.parse(self.path)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        for elem in root.getiterator():\n",
    "            if not hasattr(elem.tag, 'find'): continue  # (1)\n",
    "            i = elem.tag.find('}')\n",
    "            if i >= 0:\n",
    "                elem.tag = elem.tag[i+1:]\n",
    "        objectify.deannotate(root, cleanup_namespaces=True)\n",
    "\n",
    "        # create dict for names and xpaths of desired info from individual files\n",
    "        xml_file_elements = {\n",
    "            'filepath': './techMD/mdWrap/xmlData/object/originalName',\n",
    "            'uuid': './techMD/mdWrap/xmlData/object/objectIdentifier/objectIdentifierValue',\n",
    "            'hashtype': './techMD/mdWrap/xmlData/object/objectCharacteristics/fixity/messageDigestAlgorithm',\n",
    "            'hashvalue': './techMD/mdWrap/xmlData/object/objectCharacteristics/fixity/messageDigest',\n",
    "            'bytes': './techMD/mdWrap/xmlData/object/objectCharacteristics/size',\n",
    "            'format': './techMD/mdWrap/xmlData/object/objectCharacteristics/format/formatDesignation/formatName',\n",
    "            'version': './techMD/mdWrap/xmlData/object/objectCharacteristics/format/formatDesignation/formatVersion',\n",
    "            'puid': './techMD/mdWrap/xmlData/object/objectCharacteristics/format/formatRegistry/formatRegistryKey',\n",
    "            'modified_date': './techMD/mdWrap/xmlData/object/objectCharacteristics/creatingApplication/dateCreatedByApplication',\n",
    "            'fits_modified_unixtime': './techMD/mdWrap/xmlData/object/objectCharacteristics/objectCharacteristicsExtension/fits/fileinfo/fslastmodified[@toolname=\"OIS File Information\"]',\n",
    "            }\n",
    "\n",
    "        # build xml document root\n",
    "        mets_root = root\n",
    "\n",
    "        # gather info for each file in filegroup \"original\"\n",
    "        for target in mets_root.findall(\".//fileGrp[@USE='original']/file\"):\n",
    "\n",
    "            original_file_count += 1\n",
    "\n",
    "            # create new dictionary for this item's info\n",
    "            file_data = dict()\n",
    "\n",
    "            # create new list of dicts for premis events in file_data\n",
    "            file_data['premis_events'] = list()\n",
    "\n",
    "            # gather amdsec id from filesec\n",
    "            amdsec_id = target.attrib['ADMID']\n",
    "            file_data['amdsec_id'] = amdsec_id\n",
    "                \n",
    "            # parse amdSec \n",
    "            amdsec_xpath = \".//amdSec[@ID='{}']\".format(amdsec_id)\n",
    "            for target1 in mets_root.findall(amdsec_xpath):\n",
    "                \n",
    "                # iterate over elements and write key, value for each to file_data dictionary\n",
    "                for key, value in xml_file_elements.items():\n",
    "                    try:\n",
    "                        file_data['{}'.format(key)] = target1.find(value).text\n",
    "                    except AttributeError:\n",
    "                        file_data['{}'.format(key)] = ''\n",
    "\n",
    "                # parse premis events related to file\n",
    "                premis_event_xpath = \".//digiprovMD/mdWrap[@MDTYPE='PREMIS:EVENT']\"\n",
    "                for target2 in target1.findall(premis_event_xpath):\n",
    "\n",
    "                    # create dict to store data\n",
    "                    premis_event = dict()\n",
    "\n",
    "                    # create dict for names and xpaths of desired elements\n",
    "                    premis_key_values = {\n",
    "                        'event_uuid': './xmlData/event/eventIdentifier/eventIdentifierValue', \n",
    "                        'event_type': '.xmlData/event/eventType', \n",
    "                        'event_datetime': './xmlData/event/eventDateTime', \n",
    "                        'event_detail': './xmlData/event/eventDetail', \n",
    "                        'event_outcome': './xmlData/event/eventOutcomeInformation/eventOutcome', \n",
    "                        'event_detail_note': './xmlData/event/eventOutcomeInformation/eventOutcomeDetail/eventOutcomeDetailNote'\n",
    "                    }\n",
    "\n",
    "                    # iterate over elements and write key, value for each to premis_event dictionary\n",
    "                    for key, value in premis_key_values.items():\n",
    "                        try:\n",
    "                            premis_event['{}'.format(key)] = target2.find(value).text\n",
    "                        except AttributeError:\n",
    "                            premis_event['{}'.format(key)] = ''\n",
    "\n",
    "                    # write premis_event dict to file_data\n",
    "                    file_data['premis_events'].append(premis_event)\n",
    "\n",
    "            # format filepath\n",
    "            file_data['filepath'] = file_data['filepath'].replace('%transferDirectory%', '')\n",
    "            file_data['filepath'] = file_data['filepath'].replace('data/objects/', '')\n",
    "            file_data['filepath'] = file_data['filepath'].replace('objects/', '')\n",
    "            file_data['filename'] = os.path.basename(file_data['filepath'])\n",
    "\n",
    "            # format PUID\n",
    "            if not 'fido' in file_data['puid'].lower():\n",
    "                file_data['puid'] = \"<a href=\\\"http://nationalarchives.gov.uk/PRONOM/%s\\\" target=\\\"_blank\\\">%s</a>\" % (file_data['puid'], file_data['puid'])\n",
    "\n",
    "            # create human-readable size\n",
    "            file_data['bytes'] = int(file_data['bytes'])\n",
    "            file_data['size'] = '0 bytes' # default to none\n",
    "            if file_data['bytes'] != 0:\n",
    "                file_data['size'] = convert_size(file_data['bytes'])\n",
    "\n",
    "            # create human-readable version of last modified Unix time stamp if file was characterized by FITS\n",
    "            if file_data['fits_modified_unixtime']:\n",
    "                unixtime = int(file_data['fits_modified_unixtime'])/1000 # convert milliseconds to seconds\n",
    "                file_data['modified_unix_timestamp'] = datetime.datetime.fromtimestamp(unixtime).isoformat() # convert from unix to iso8601\n",
    "\n",
    "            # append file_data to original files\n",
    "            original_files.append(file_data)\n",
    "\n",
    "        # gather dublin core metadata from most recent dmdSec\n",
    "        dc_metadata = self.parse_dc(root)\n",
    "\n",
    "        # print(\"original_files\", original_files)\n",
    "\n",
    "        # print(\"dc_metadata\", dc_metadata)\n",
    "\n",
    "        # print(\"original_file_count\", original_file_count)\n",
    "\n",
    "        self.original_files = original_files\n",
    "        self.dc_metadata = dc_metadata\n",
    "        self.original_file_count = original_file_count\n",
    "\n",
    "        self.mets_root = mets_root\n",
    "\n",
    "        # add file info to database\n",
    "        # mets_instance = METS(mets_filename, self.nickname, original_files, dc_metadata, original_file_count)\n",
    "        # db.session.add(mets_instance)\n",
    "        # db.session.commit()\n",
    "\n",
    "    def get_original_files(self):\n",
    "        data = self.original_files\n",
    "        df = pd.DataFrame([{k: v for k, v in d.items() if k != 'premis_events'} for d in data])\n",
    "        return df\n",
    "    \n",
    "    def get_file_format_counts(self):\n",
    "        data = self.original_files\n",
    "        df = pd.DataFrame(data)\n",
    "        file_format_counts = df['format'].value_counts()\n",
    "        return file_format_counts\n",
    "\n",
    "    def visualize_file_format_counts(self):\n",
    "        file_format_counts = self.get_file_format_counts()\n",
    "        # 円グラフを作成\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        file_format_counts.plot(kind='pie', autopct='%1.1f%%', startangle=90, counterclock=False)\n",
    "        plt.title('File Count by Format')\n",
    "        plt.ylabel('')  # y軸ラベルを非表示\n",
    "        plt.show()\n",
    "\n",
    "    def visualize_file_events_count(self):\n",
    "        data = self.original_files\n",
    "        # 集計: ファイルごとのイベントタイプの数をカウント\n",
    "        file_events_count = {}\n",
    "        for entry in data:\n",
    "            filename = entry['filename']\n",
    "            event_types = [event['event_type'] for event in entry['premis_events']]\n",
    "            file_events_count[filename] = pd.Series(event_types).value_counts().to_dict()\n",
    "\n",
    "        # DataFrameに変換して表示\n",
    "        df = pd.DataFrame(file_events_count).fillna(0)\n",
    "\n",
    "        # プロットの準備\n",
    "        df.plot(kind='bar', stacked=True, figsize=(10, 6))\n",
    "        plt.title('Event Type Count per File')\n",
    "        plt.xlabel('Event Type')\n",
    "        plt.ylabel('Count')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    ns = {\n",
    "        'mets': 'http://www.loc.gov/METS/',\n",
    "        'xlink': 'http://www.w3.org/1999/xlink'\n",
    "    }\n",
    "\n",
    "    def parse_file_sec(self):\n",
    "        mets_root = self.mets_root\n",
    "\n",
    "        file_data = []\n",
    "\n",
    "        # Namespace dictionary to handle the xlink namespace\n",
    "        ns = self.ns\n",
    "\n",
    "        # Iterate over each fileGrp and extract relevant data\n",
    "        for fileGrp in mets_root.find('fileSec', '').findall('fileGrp', ns):\n",
    "            use = fileGrp.attrib.get('USE', '')\n",
    "            for file in fileGrp.findall('file', ns):\n",
    "                file_id = file.attrib.get('ID', '')\n",
    "                group_id = file.attrib.get('GROUPID', '')\n",
    "                admid = file.attrib.get('ADMID', '')\n",
    "                for flocat in file.findall('FLocat', ns):\n",
    "                    href = flocat.attrib.get('{http://www.w3.org/1999/xlink}href', '')\n",
    "                    loctype = flocat.attrib.get('LOCTYPE', '')\n",
    "                    otherloctype = flocat.attrib.get('OTHERLOCTYPE', '')\n",
    "                    # Append data to the list\n",
    "                    file_data.append({\n",
    "                        'USE': use,\n",
    "                        'File ID': file_id,\n",
    "                        'Group ID': group_id,\n",
    "                        'ADMID': admid,\n",
    "                        'File Location': href,\n",
    "                        'LOCTYPE': loctype,\n",
    "                        'OTHERLOCTYPE': otherloctype\n",
    "                    })\n",
    "\n",
    "        # Create a pandas DataFrame from the extracted data\n",
    "        df = pd.DataFrame(file_data)\n",
    "\n",
    "        return df\n",
    "    \n",
    "    # 再帰的にmets:structMapを可視化する関数\n",
    "    def print_structMap(self, div, level=0, is_last=False, prefix=\"\"):\n",
    "\n",
    "        ns = self.ns\n",
    "\n",
    "        # ツリー表示用の線のパターンを設定\n",
    "        branch = \"└── \" if is_last else \"├── \"\n",
    "        space = \"    \" if is_last else \"│   \"\n",
    "\n",
    "        # 階層に応じてインデントを作成し、ツリーの分岐を作成\n",
    "        indent = f\"{prefix}{branch}\"\n",
    "        \n",
    "        # divのタイプとラベルを表示\n",
    "        div_type = div.attrib.get('TYPE', '')\n",
    "        div_label = div.attrib.get('LABEL', '')\n",
    "        print(f\"{indent}{div_type}: {div_label}\")\n",
    "        \n",
    "        # 子のmets:div要素を再帰的に処理\n",
    "        children = div.findall('div', ns)\n",
    "        for i, child_div in enumerate(children):\n",
    "            is_last_child = (i == len(children) - 1)\n",
    "            self.print_structMap(child_div, level + 1, is_last_child, prefix + space)\n",
    "\n",
    "    def visualize_structMap(self):\n",
    "\n",
    "        mets_root = self.mets_root\n",
    "\n",
    "        ns = self.ns\n",
    "\n",
    "        # mets:structMapを取得して可視化\n",
    "        for struct_map in mets_root.findall('structMap', ns):\n",
    "            struct_map_type = struct_map.attrib.get('TYPE', '')\n",
    "            struct_map_label = struct_map.attrib.get('LABEL', '')\n",
    "            print(f\"StructMap (TYPE: {struct_map_type}, LABEL: {struct_map_label})\")\n",
    "\n",
    "            # ルートのmets:div要素を処理\n",
    "            root_divs = struct_map.findall('div', ns)\n",
    "            for i, div in enumerate(root_divs):\n",
    "                is_last_div = (i == len(root_divs) - 1)\n",
    "                self.print_structMap(div, is_last=is_last_div)\n",
    "\n",
    "    def show_file_changes(self):\n",
    "\n",
    "        log_file_path = self.path\n",
    "    \n",
    "        # ログファイルのパスを指定\n",
    "        # log_file_path = \"change.log\"\n",
    "        \n",
    "        # ファイル名の変更を示す行を抽出する正規表現\n",
    "        change_pattern = r'Changed name:\\s*(.*)\\s*->\\s*(.*)'\n",
    "        \n",
    "        # ファイル名の変更を格納するリスト\n",
    "        file_changes = []\n",
    "        \n",
    "        # ログファイルを読み込んで処理\n",
    "        with open(log_file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                # ファイル名の変更を示す行を正規表現で抽出\n",
    "                match = re.search(change_pattern, line)\n",
    "                if match:\n",
    "                    old_name = match.group(1).strip()\n",
    "                    new_name = match.group(2).strip()\n",
    "                    file_changes.append((old_name, new_name))\n",
    "        \n",
    "        # 結果を見やすく表示\n",
    "        if file_changes:\n",
    "            print(\"ファイル名の変更一覧:\")\n",
    "            for old_name, new_name in file_changes:\n",
    "                print(f\"- 変更前: {old_name.split('/')[-1]}\")\n",
    "                print(f\"  変更後: {new_name.split('/')[-1]}\")\n",
    "                print(\"-\" * 40)\n",
    "        else:\n",
    "            print(\"ファイル名の変更は見つかりませんでした。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
